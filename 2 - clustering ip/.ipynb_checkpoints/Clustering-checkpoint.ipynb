{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627666, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J.TOMSON Womens Basic Long Sleeve Cardigan Swe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tommy Hilfiger Womens Full Zip Hooded Logo Swe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life is good Women's More Peace Go-To Zip Hoodie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bepei Women's Long Sleeve Crewneck Tunic Sweat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H.X Girls' Ballet Dance Knit Wrap Long Sleeve ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  J.TOMSON Womens Basic Long Sleeve Cardigan Swe...\n",
       "1  Tommy Hilfiger Womens Full Zip Hooded Logo Swe...\n",
       "2   Life is good Women's More Peace Go-To Zip Hoodie\n",
       "3  Bepei Women's Long Sleeve Crewneck Tunic Sweat...\n",
       "4  H.X Girls' Ballet Dance Knit Wrap Long Sleeve ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "#df = pd.read_csv('/Users/altay.amanbay/Desktop/Dining & Entertaining > Plates - 620.csv')\n",
    "#df = pd.read_csv('/Users/altay.amanbay/Desktop/3 - Clustered data/raw/raw_trainset_[pods&sheets_657]_w_amzn.csv')\n",
    "df = pd.read_csv('/Users/altay.amanbay/Desktop/140-657399.csv')\n",
    "\n",
    "df.rename(columns={'\\ufeff\"category_id\"': 'category_id'}, inplace=True)\n",
    "df.rename(columns={'\\ufeff\"description_mod1\"': 'description'}, inplace=True)\n",
    "df = df.loc[:,['description']]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627666, 1)\n"
     ]
    }
   ],
   "source": [
    "# Drop NA rows\n",
    "\n",
    "df.dropna(inplace=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627666, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature vector matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(min_df=1, ngram_range=(1,1), max_features = 1000, binary = False, stop_words=\"english\")\n",
    "tfidf_vect = TfidfVectorizer(sublinear_tf=True, max_df=1.0, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "count_vect_matrix = count_vect.fit_transform(df.loc[:,'description'])\n",
    "count_vect_matrix = count_vect_matrix.toarray()\n",
    "\n",
    "# count_vect_matrix = tfidf_vect.fit_transform(df.loc[:,'description'])\n",
    "# count_vect_matrix = count_vect_matrix.toarray()\n",
    "\n",
    "# count_vect_matrix_t = StandardScaler().fit_transform(count_vect_matrix)\n",
    "\n",
    "print(count_vect_matrix.shape)\n",
    "count_vect_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "ms = MeanShift(n_jobs = -1)\n",
    "ms.fit(count_vect_matrix)\n",
    "clusters = ms.labels_\n",
    "\n",
    "end = time.time()\n",
    "print('MeanShift clustering took %g s' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n = 6276\n",
    "kmeans = KMeans(n_clusters=n, n_jobs=-1)\n",
    "kmeans.fit(count_vect_matrix)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "print('Unique labels:',Counter(clusters))\n",
    "\n",
    "end = time.time()\n",
    "print('KMeans clustering took %g s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cluster'] = pd.Series(clusters)\n",
    "df.head(2)\n",
    "df.to_csv('/Users/altay.amanbay/Desktop/dbscan.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini Batch KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n = 6276\n",
    "batch_size_ = 10000\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=n, batch_size=batch_size_)\n",
    "mbk.fit(count_vect_matrix)\n",
    "clusters = mbk.labels_\n",
    "\n",
    "print('Unique labels:',Counter(clusters))\n",
    "\n",
    "end = time.time()\n",
    "print('KMeans clustering took %g s' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import mmwrite\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kirkland Signature Ultra Clean, Re...\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity(s1, s2):\n",
    "    return 1.0 - (0.01 * max(\n",
    "        fuzz.ratio(s1, s2),\n",
    "        fuzz.token_sort_ratio(s1, s2),\n",
    "        fuzz.token_set_ratio(s1, s2)))\n",
    "        \n",
    "\n",
    "cutoff = 2\n",
    "stitles = []\n",
    "#fin = open(os.path.join(OUTPUT_DIR, \"stitles.txt\"), 'rb')\n",
    "#for line in fin:\n",
    "for index, line in df.iterrows():\n",
    "    #stitle, count = line.strip().split(\"\\t\")\n",
    "    stitle = line['description'] #.strip().split(' ')\n",
    "    #if int(count) < cutoff:\n",
    "    #    continue\n",
    "    stitles.append(stitle)\n",
    "#fin.close()\n",
    "\n",
    "print(stitles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "X = np.zeros((len(stitles), len(stitles)))\n",
    "for i in range(len(stitles)):\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        print(\"Processed %d/%d rows of data\" % (i, X.shape[0]))\n",
    "    for j in range(len(stitles)):\n",
    "        if X[i, j] == 0.0:        \n",
    "            X[i, j] = compute_similarity(stitles[i].lower(), stitles[j].lower())\n",
    "            X[j, i] = X[i, j]\n",
    "            \n",
    "end = time.time()\n",
    "print('Similarity computation took %g s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to Matrix Market format for passing to DBSCAN\n",
    "OUTPUT_DIR = '/Users/altay.amanbay/Desktop'\n",
    "mmwrite(os.path.join(OUTPUT_DIR, \"stitles.mtx\"), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate distance matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(count_vect_matrix)\n",
    "#dist\n",
    "#np.round(dist, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering took 1.29487 s\n",
      "Unique labels: Counter({2: 7732, -1: 1981, 16: 45, 64: 32, 9: 29, 3: 26, 10: 26, 83: 25, 62: 23, 1: 22, 5: 19, 12: 19, 45: 19, 79: 19, 71: 18, 58: 17, 18: 16, 34: 16, 50: 16, 90: 16, 7: 15, 15: 15, 30: 15, 87: 15, 61: 14, 80: 14, 24: 13, 73: 12, 27: 11, 23: 10, 75: 10, 76: 10, 4: 9, 17: 9, 32: 9, 51: 9, 99: 9, 106: 9, 13: 8, 14: 8, 38: 8, 40: 8, 47: 8, 65: 8, 74: 8, 78: 8, 84: 8, 86: 8, 105: 8, 33: 7, 42: 7, 46: 7, 59: 7, 63: 7, 81: 7, 93: 7, 97: 7, 98: 7, 102: 7, 0: 6, 6: 6, 11: 6, 20: 6, 22: 6, 25: 6, 26: 6, 29: 6, 31: 6, 35: 6, 36: 6, 37: 6, 41: 6, 48: 6, 49: 6, 54: 6, 60: 6, 68: 6, 69: 6, 72: 6, 85: 6, 88: 6, 100: 6, 103: 6, 8: 5, 19: 5, 28: 5, 39: 5, 43: 5, 44: 5, 52: 5, 53: 5, 55: 5, 56: 5, 57: 5, 67: 5, 70: 5, 82: 5, 89: 5, 91: 5, 92: 5, 94: 5, 101: 5, 104: 5, 66: 4, 77: 4, 95: 4, 21: 3, 96: 3})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "db = DBSCAN(eps=0.5, min_samples=5, metric=\"precomputed\", n_jobs = -1).fit(dist)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "end = time.time()\n",
    "print('DBSCAN clustering took %g s' % (end - start))\n",
    "\n",
    "print('Unique labels:',Counter(labels))\n",
    "#print(labels[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['cluster'] = pd.Series(labels)\n",
    "df.head(2)\n",
    "df.to_csv('/Users/altay.amanbay/Desktop/dbscan.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering exmple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "\n",
    "X_t = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "print(np.unique(labels))\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distance matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(count_vect_matrix)\n",
    "#dist\n",
    "#np.round(dist, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the linkage_matrix using ward (hClust) clustering pre-computed distances\n",
    "from scipy.cluster.hierarchy import ward, is_valid_linkage\n",
    "\n",
    "linkage_matrix = ward(dist) \n",
    "linkage_matrix\n",
    "#is_valid_linkage(linkage_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import cluster\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "Z = cluster.hierarchy.ward(count_vect_matrix)\n",
    "#cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[5, 10])\n",
    "\n",
    "end = time.time()\n",
    "print('WARD clustering took %g s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot dendrogram\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=('a','b'));\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#uncomment below to save figure\n",
    "#plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.        ,   9.        ,   2.44948974,   2.        ],\n",
       "       [  1.        ,   6.        ,   2.64575131,   2.        ],\n",
       "       [  0.        ,  10.        ,   3.16227766,   3.        ],\n",
       "       [  7.        ,  12.        ,   3.46410162,   4.        ],\n",
       "       [  8.        ,  13.        ,   3.57770876,   5.        ],\n",
       "       [  2.        ,  14.        ,   3.81226092,   6.        ],\n",
       "       [  3.        ,  15.        ,   4.15187852,   7.        ],\n",
       "       [ 11.        ,  16.        ,   4.51979772,   9.        ],\n",
       "       [  5.        ,  17.        ,   4.83735465,  10.        ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(count_vect_matrix, 'ward')\n",
    "cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[7, 10])\n",
    "cutree\n",
    "\n",
    "c, coph_dists = cophenet(Z, pdist(count_vect_matrix))\n",
    "#coph_dists\n",
    "Z\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
